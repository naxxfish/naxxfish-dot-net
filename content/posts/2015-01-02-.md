---
title: Radio Broadcast Coverage in a Online Mobile World
author: naxxfish
type: posts
date: -001-11-30T00:00:00+00:00
draft: true
url: /?p=192
categories:
  - Uncategorised

---
It used to be that a RF simulation and survey would give you a good idea of where people can listen to your broadcasts and, with geographic population data, roughly who your audience is.

As we are relentlessly being told, listening via mobile phones is on the increase , from 25% increase year on year for 15-25 year olds to a 45% increase for adults year on year  &ndash; including both mobile phones and tablets &ndash; and online listening across all devices is increasing the most out of all digital platforms (12% since Q3 2013)_ ([Q3 2014 infographic][1] RAJAR/Ipsos MORI/RSMB).  _

This makes sense &ndash; the world has moved on since the advent of the iPod and other specialist music devices and towards the reality of a smartphone which can handle not just your media but a host of other tasks as well, in one small (well, mostly small) package.

But, then, seeing as your broadcasts are being consumed via a medium (the Internet) which now effectively removes your geographic constraints &ndash; how do you know where your audience is, and whether you're serving their needs effectively?

<!--more-->

To be clear, this is markedly different to the situation where broadcasters have wide ranging coverage &ndash; like national or international broadcasters. For those select few, they can still be well assured that their reach is as far as their transmissions go.  Additionally, they can take a good guess at the quality of the reception in any given area by simulating the radio propagation from their transmitters using geographic and (sometimes) atmospheric data, as well as interference from other local sources.

For Online listeners, even big broadcasters are not completely sure of the quality that their service is being received, as it no longer depends on their own equipment &ndash; it depends upon countless other networks and systems that carry their &#8220;signal&#8221; to the listener.  It's completely possible that two listeners, in the exact same location, will have different experiences due to the device, network coverage and network conditions that they are each subject to. To make matters worse, these conditions are not static.  Network conditions can change on a second by second basis. Mobile reception can change depending on how many users there are on the cell.  With some wireless technologies, even the velocity (or mobility) of the listener can change their ability to receive a stream ([[2]][2]).  Even how much the listener pays per month (i.e. what service plan they are on) to their network operator could have an effect.

This confusion brings about a hefty bunch of questions. Lets try to extract a few:

**What do we mean by quality?**

First of all, lets remove the content part of the equation.  That's a far more difficult problem which we'll leave to the creative types who can do a far better job than any scientific analysis can.

In the days of &#8220;real&#8221; broadcasting, you could go out in a reception van with an array of antennae simulating typical devices used by the audience to tune into your service.  You could then measure the received signal and compare it against a reference of some kind (whether that's a reference signal from your transmitter, or a more general reference of what a listener expects in terms of their perception of the content).  You could characterise different departures from the ideal &ndash; for example with radio broadcasts you might have [fading][3], [intermod interference][4], [multipath interference][5], even electrical interference from local sources. You could plot these on a map &ndash; and you could be reasonably sure, after doing a few different samples at different times and some statistical analysis, that these measurements will be a good predictor of the quality of the signal in the future.

For online streaming, a different approach is required.

**Network based metrics**

With online &#8220;streaming&#8221;, we have a different set of possible degradation types.  For the purposes of this discussion &ndash; those will be confined to degradation that is introduced at the point of distribution of the encoded stream &ndash; we will assume the codec is working entirely properly.

First, we need to examine how streaming media is consumed at the receiver.

[<img class="alignnone size-full wp-image-195" src="https://i0.wp.com/vandium.naxxfish.net/wp-content/uploads/2015/01/Online-Reception-Outline.png?resize=640%2C96" alt="Online Reception Outline" width="640" height="96" srcset="https://i0.wp.com/naxxfish.net/wp-content/uploads/2015/01/Online-Reception-Outline.png?w=661&ssl=1 661w, https://i0.wp.com/naxxfish.net/wp-content/uploads/2015/01/Online-Reception-Outline.png?resize=300%2C45&ssl=1 300w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1" />][6]

Generally speaking, there are a couple of major blocks that do the required work. In order to characterise how the process may fail (and degrade quality) we need to look at how the process works.

There will be a process of some sort which receives data from the **network**.  This could be any method &ndash; TCP/IP or otherwise.  Data which has been extracted from the network is then pushed into a **buffer** &ndash; or a First in Last out queue of dynamic size, if you like. The next process then takes data from this buffer and **decodes** it, using whatever codec is being used for this stream. After it's decoded, it's then sent to the **playback** device.

In a good quality scenario, a steady stream of data is received, piled into the buffer and decoded at about the same rate, and played back providing a gap-free listening experience.

In real life over a real network, a few things can mess this up:

_[Packet Loss][7] _means that packets of data that are part of the stream never make it to the receiver.  Uncorrected, this can at worst disrupt the decoder and cause it to &#8220;skip&#8221; past the audio which it did not receive. If the packet loss continues, eventually the decoder will decode the data in the buffer faster than it receives new data &ndash; and cause a buffer underrun, which in turn causes the output to cease entirely.  Arguably, intermittent &#8220;skipping&#8221; of content is worse than a complete loss of the stream.  Packet loss is correctable via a number of means, but most of them will introduce&#8230;

_[Jitter ][8]&ndash;_ causes a situation where data arrives at an inconsistent rate.

One way of looking at jitter is by imagining the packets are like public transport &ndash; specifically buses.  The timetable says that the bus should arrive at the terminus every 10 minutes &ndash; however some buses were delayed slightly, and others ran a little quicker than planned.  You then end up with the time between buses being different &ndash; there might be a 5 minute gap between some, 15 minutes by another.

On the Internet, some jitter is expected &ndash; this is one of the main reasons for having a buffer &ndash; the decoder can work it's way through the buffer and be given a consistent stream of data.

Going back to the bus analogy, this might be like having a queue of buses at a terminus. If one bus departs every 10 minutes, and one should arrive every 10 minutes, and you keep at least 3 buses at the terminus at all times, then you can tolerate some buses arriving late and some early and continue to have them depart at the same interval. The only thing required to have a bus depart on time is that there's at least one left in the terminus.

However &ndash; what if after normal running, suddenly 4 consecutive buses are delayed by 20 minutes each?

  * Start: 3 buses in the queue. One bus departs, the first bus does NOT arrive &ndash; you now have 2 buses in the queue
  * 10 minutes: 2 buses in the queue &ndash; one departs, but the first bus arrives &ndash; you still have 2 buses in the queue
  * 20 minutes: 2 buses in the queue &ndash; another one departs, but the second bus is still 10 minutes away &ndash; you have 1 bus in the queue!
  * 30 minutes: 1 bus in the queue, which departs.  The second bus arrives, leaving you with 1 bus in the queue.
  * 40 minutes: 1 bus in the queue, which departs. The third bus is still 10 minutes away &ndash; so you have no more busses!
  * 50 minutes: No buses in the queue! But, thankfully, the third one arrives which departs immediately.
  * 60 minutes: Still no buses in the queue, and the fourth bus is still 10 minutes away. You have to cancel the bus due to depart now, and deal with the wrath of the commuters.

This is a buffer underrun. You've run out of buses. Here, what we've actually seen is also an illustration of _latency_ &ndash; that is, the time that it takes for a packet to arrive at it's destination.  In this example, the latency is greater than the rate at which the packets are sent (assuming they're sent every 10 minutes).  _Jitter_ is the measure of variation of latency.

As the example above, the effect of sudden increased latency could be reduced by increasing the number of buses kept at the terminus (or increasing the buffer size).   However, this only works for spikes in latency.  If we care about the order in which data arrives (as we do for streaming media), then if the content arrives out of order because some packets have a much larger latency than others &ndash; then other methods must be used to reduce the effect. You would need a larger buffer, and a way of re-ordering the packets on arrival too.  To take into account total losses, you would also need a mechanism by which you know when to give up waiting for the packet to arrive and continue on with the packets you've already got.

There are various ways of coping with these different kinds of network service degradation. But these things on their own are not necessarily equal to degradation of content quality.   Rather, they are possible &ndash; but not certain &ndash; causes.

So it's not enough to measure these metrics individually.  Whilst you can imagine that a high latency and a high jitter will be problematic for many receivers, some may cope better than others. What we actually want to measure is the listener experience.

**Listener (perceptual) based metrics**

Lets define what a listener would expect to be a perfect listening experience.

> As a listener, I want to be able to listen to the content without any **interruption**, **omission **or **distortion**.

Here, an interruption might be defined as: a loss of content, or a &#8220;pause&#8221; in the stream. An omission might be defined as content which is missing &ndash; and &#8220;skipped&#8221; over. Distortion is any artifact introduced by a stream containing corrupted data, for whatever reason.

The difference between listener based metrics and more traditional technical metrics is that you are treating the entire distribution mechanism as a black box, and measuring the actual output of the entire system in order to produce comparable data.

This is important since, as discussed earlier, the nature of the distribution method is vastly more dynamic &ndash; so changes in technical aspects don't always map to a appreciable change in listener experience.

This isn't a new concept &ndash; a metric called [Mean Opinion Score][9] has been used by telecommunication networks for decades to rate the quality of their voice services.   Traditionally, this was done with real humans listening to the audio in a reference room providing a scoring between 5 (excellent) and 1 (bad).  For Voice over IP installations, a recommendation exists for how to calculate the score using an algorithmic process &ndash; the Perceptual Evaluation of Speech Quality (PESQ) ([ITU P.862][10]).  Contained within the ITU recommendation is one of my favorite systems diagrams published.

[<img class="alignnone wp-image-196 size-full" src="https://i2.wp.com/vandium.naxxfish.net/wp-content/uploads/2015/01/PESQ.png?resize=640%2C339" alt="PESQ" width="640" height="339" srcset="https://i1.wp.com/naxxfish.net/wp-content/uploads/2015/01/PESQ.png?w=1026&ssl=1 1026w, https://i1.wp.com/naxxfish.net/wp-content/uploads/2015/01/PESQ.png?resize=300%2C159&ssl=1 300w, https://i1.wp.com/naxxfish.net/wp-content/uploads/2015/01/PESQ.png?resize=768%2C407&ssl=1 768w, https://i1.wp.com/naxxfish.net/wp-content/uploads/2015/01/PESQ.png?resize=1024%2C543&ssl=1 1024w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1" />][10]

&nbsp;

I, for one, welcome our geometric overlords.[*][11]

The drawback here is this is geared towards the evaluation of narrow band speech using codecs designed for such (GSM, u/alaw etc) . Internet broadcasters don't tend to use these kinds of codecs, instead preferring wideband codecs to produce the most accurate reproduction possible for a wide variety of types of programmes (e.g. MPEG-2, MPEG Layer 3, AAC &#8230;).

There's a whole world of analysis into how to [qualitatively measure the &#8220;quality&#8221; of audio][12] &ndash; but lets not go down that particular rabbit hole. What we're looking for here is something much more basic.

> We, as broadcasters, want to know if our audience can receive the audio and effectively listen to it for a reasonable period of time.

This is where the research is needed.  Exactly how much &#8220;skipping&#8221; and &#8220;pausing&#8221; limits your listening ability? One might say that any skipping or pausing would be enough to prevent you from listening effectively &ndash; but another might argue that a 0.1ms skip in audio over a 30 minute listening session might be tolerable.

Speech content tends to have quite a lot of silence in it (between words and sentences), but music might not.  If that 0.1ms skip happened on a speech programme between sentences, you may not even notice it &ndash; but if it happened in the middle of a particularly energetic passage of music, it could ruin the experience entirely.

What we can agree on, is that the perception is dependent on those three things &ndash; &#8220;skips&#8221;, &#8220;pauses&#8221; and &#8220;distortions&#8221;. Those are things that we can measure, that will directly relate to the perception of quality.

**Mapping the world**

Now we know what we're going to need to measure, how we need to work out how to measure it. To get the perfect picture, you would need to take measurements at every point in the area of interest, using every ISP and network technology (3G, 4G, WiMAX, PSTN Dialup (56k), ISDN, ADSL, VDSL, DOCSIS etc.) that is currently available, using every means that someone may listen to the service.

Obviously, this is outside the realms of practical reality.

Instead, you could get some statistical data and work out the most popular network operators, ISPs and technologies and use those to capture the majority of your audience. If you were particularly switched on, you could even look at your streaming listener logs (which you normally have to keep for PPL music licensing purposes anyway) to find out what the most popular services are and make sure you're measuring those.

You'd then have to develop some sort of device that will be able to take the measurements, using all of the means by which you broadcast (different codecs, quality levels) given any normal network connection. The device would have to have resources comparable to that of a standard device (at the time), and be able to replicate any internal differences in processing that arise due to self imposed constraints (e.g. reduce CPU clock speed for &#8220;power saving&#8221; modes)

For fixed broadband, the geographic differences between services is pretty minimal, ruling out local and back haul problems &ndash; it's the ISPs network itself that has the widest reaching effect. So you could get away with installing your own lines at your office, hooked up to some devices do perform the testing automatically on each of the connections.

Then, to measure vehicular mobile experience, you can run around with a good old reception van, a handful of 3G/4G modems, and a GPS receiver,then drive around measuring how well the devices do at receiving your stream.  They'd have to be configured in all possible ways that the devices could be (different modes, different data plans, etc). You'd then have to drive across the whole area &ndash; at a fixed speed (so that that does not add a new variable to the measurement).

Finally, to measure pedestrian mobile experience, you could find a handful of volunteers, and give them a backpack full of computer and GPS , then send them wandering around your target area until you get a full coverage map.

That's rather a lot to do, really, isn't it?

**Practicaly mapping your coverage**

Centralised empirical methods aside, is there any way in which you could estimate your mobile internet stream &#8220;coverage&#8221;?

Maybe &ndash; [RootMetrics produce coverage maps][13] for the primary mobile network operators based on consumer oriented testing (see [their Methodology][14]).  They use unmodified Android smartphones to do their testing, using a custom application to perform a battery of tests, driven and walked around by &#8220;scouts&#8221; to collect their data. They then publish a calculated RootScore based on their results which are used for comparison purposes between operators.   Similarly [OpenSignal][15] create similar reports and collect their data entirely crowd-based, where users install an app on their phone which gathers data about coverage and forwards it on to their data collection facilities.

There's no reason why you couldn't use a similar method to these apps to measure extra metrics (jitter, packet loss, latency), rather than just up and download speeds and &#8220;ping&#8221; time.

As mentioned, these measurements on their own are not enough &ndash; what is then required is for those measurements to convert those into a real listening experience score.

One way of doing this might be to have the metrics replicated by a network simulator, and then the stream passed through it before being received and decoded.  Once that has been done, you could then analyse the quality of the stream at the exact time that the measurement was taken to determine what the listening quality would have been, if a listener had been there.

Alternatively, you could have the app do this itself &ndash; although that would require using a fair bit of the user's data allowance to stream the content.

&nbsp;

 [1]: http://www.rajar.co.uk/docs/news/RAJAR_DataRelease_InfographicQ32014.pdf
 [2]: http://people.cs.umass.edu/~yungchih/publication/12_mtcp_4g_tech_report.pdf
 [3]: http://en.wikipedia.org/wiki/Fading
 [4]: http://en.wikipedia.org/wiki/Intermodulation
 [5]: http://en.wikipedia.org/wiki/Multipath_interference
 [6]: https://i0.wp.com/vandium.naxxfish.net/wp-content/uploads/2015/01/Online-Reception-Outline.png
 [7]: http://en.wikipedia.org/wiki/Packet_loss
 [8]: http://en.wikipedia.org/wiki/Jitter
 [9]: http://en.wikipedia.org/wiki/Mean_opinion_score
 [10]: http://www.itu.int/rec/T-REC-P.862
 [11]: http://knowyourmeme.com/memes/i-for-one-welcome-our-new-insect-overlords
 [12]: https://pure.ltu.se/portal/files/41430750/Dan_Nyberg.pdf
 [13]: http://webcoveragemap.rootmetrics.com/uk
 [14]: http://www.rootmetrics.com/uk/methodology
 [15]: http://opensignal.com/